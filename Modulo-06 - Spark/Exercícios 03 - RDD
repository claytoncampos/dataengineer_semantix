1. Ler com RDD os arquivos localmente do diretório “/opt/spark/logs/” ("file:///opt/spark/logs/")
log = sc.textFile("file:///opt/spark/logs/")
2. Com uso de RDD faça as seguintes operações

a) Contar a quantidade de linhas
log.count()

b) Visualizar a primeira linha
log.first()

c) Visualizar todas as linhas
log.collect()

d) Contar a quantidade de palavras
#separar as palavras das linhas

palavras = log.flatMap(lambda linha: linha.split(" "))
#contas as palavras
palavras.count()


e) Converter todas as palavras em minúsculas
minuscula = palavras.map(lambda palavra: palavra.lower())

f) Remover as palavras de tamanho menor que 2
minuscula_maior2 = minuscula.filter(lambda palavra: len(palavra) > 2)


g) Atribuir o valor de 1 para cada palavra
palavra_1 = minuscula_maior2.map(lambda palavra: (palavra, 1))

h) Contar as palavras com o mesmo nome
palavras_reduce = palavra_1.reduceByKey(lambda chave1, chave2 : chave1 + chave2)

i) Visualizar em ordem alfabética
palavras_ordem = palavras_reduce.sortBy(lambda palavra: palavra[0])
palavras_ordem.collect()

j) Visualizar em ordem decrescente a quantidade de palavras
palavras_ordem_qtd = palavras_reduce.sortBy(lambda palavra: palavra[1], False)

k) Remover as palavras, com a quantidade de palavras > 1
palavras_ordem_filtro = palavras_ordem_qtd.filter(lambda palavra: palavra[1]>1)
palavras_ordem_filtro.collect()

l) Salvar o RDD no diretorio do HDFS /user/<seu-nome>/logs_count_word
palavras_ordem_filtro.saveAsTextFile("/user/clayton/logs_count_word")

3. Clicar no botão de Enviar Tarefa, e enviar o texto: ok
