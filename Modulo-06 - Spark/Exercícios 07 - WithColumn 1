1. Criar um dataframe para ler o arquivo no HDFS /user/<nome/data/juros_selic/juros_selic
juros = spark.read.csv("/user/clayton/data/juros_selic/juros_selic", sep=";", header="true")

2. Alterar o formato do campo data para “MM/dd/yyyy”
from pyspark.sql.functions import *

juros_americano_unix = juros.withColumn("data", unix_timestamp(col("data"), "dd/MM/yyyy"))
juros_americano = juros_americano_unix.withColumn("data",from_unixtime("data", "MM/dd/yyyy")).show()

ou
juros_americano = juros.withColumn("data",from_unixtime(unix_timestamp(col("data"),"dd/MM/yyyy"),"MM/dd/yyyy"))

3. Com uso da função from_unixtime crie o campo “ano_unix”, com a informação do ano do campo data

juros_unixtime = juros_americano.withColumn("ano_unix",from_unixtime(unix_timestamp(col("data"),"dd/MM/yyyy"),"yyyy"))
juros_unixtime.show(3)

4. Com uso de substring crie o campo “ano_str”, com a informação do ano do campo data
juros_substring = juros_unixtime.withColumn("ano_str", substring(col("data"),7,4))
juros_substring.show(2)

5. Com uso da função split crie o campo “ano_str”, com a informação do ano do campo data
juros_split = juros_substring.withColumn("ano_split", split(col("data"), "/").getItem(2))
juros_split.show(2)

6. Salvar no hdfs /user/rodrigo/juros_selic_americano no formato CSV, incluindo o cabeçalho
juros_split.write.csv("/user/clayton/juros_selic", header="true")
!hdfs dfs -ls /user/clayton/juros_selic


7. Clicar no botão de Enviar Tarefa, e enviar o texto: ok
